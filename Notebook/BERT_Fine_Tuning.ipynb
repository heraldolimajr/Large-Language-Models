{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heraldolimajr/Large-Language-Models/blob/main/Notebook/BERT_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exerc√≠cio de Fine Tuning do BERT com PyTorch ü§ñüêçüî•\n",
        "Neste exerc√≠cio, voc√™ ir√° implementar o fine tuning do BERT adicionando Task Head com finalidade de classifica√ß√£o. Sugest√µes utilizando a biblioteca PyTorch.\n",
        "\n",
        "Conforme visto em sala, o Self-attention √© um componente central dos Transformers, as redes neurais que impulsionam os modelos de linguagem modernos como o BERT. Ap√≥s as camadas de aten√ß√£o dos modelos, podemos adicionar uma ou mais camadas com a finalidade de executar tarefas espec√≠ficas.\n",
        "\n",
        "## Contexto\n",
        "Um modelo BERT √© um Transformer do tipo Encoder que processa um texto de entrada gerando representa√ß√µes sem√¢nticas desse texto. O Self-attention permite que o modelo determine a rela√ß√£o entre diferentes tokens em uma sequ√™ncia.\n",
        "\n",
        "Adicionaremos, conforme visto em sala de aula, Task Head ao modelo para fazer fine tuning de classifica√ß√£o.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### [<img src=\"https://colab.google/static/images/icons/colab.png\" width=100> OPCIONAL ] Configura√ß√£o do ambiente para melhor desempenho e instala√ß√£o de depend√™ncias\n",
        "üí° **Obs**: Selecione um ambiente com GPU para rodar esse notebook. No Google Colab:\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5YP62vWc1w7s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rVnQg_Gixaj-"
      },
      "outputs": [],
      "source": [
        "# Caso esteja no Google Colab ser√° necess√°rio instalar apenas as depend√™ncias abaixo\n",
        "!pip install seqeval>=1.2.2\n",
        "!pip install evaluate>=0.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importando dataset\n",
        "\n",
        "Utilizaremos o dataset Rotten Tomatoes, que cont√©m avalia√ß√µes de filmes (https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes)."
      ],
      "metadata": {
        "id": "_snCPoeF6A0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Preparando dados\n",
        "tomatoes = load_dataset(\"rotten_tomatoes\")\n",
        "# Separando\n",
        "train_data, test_data = tomatoes[\"train\"], tomatoes[\"test\"]"
      ],
      "metadata": {
        "id": "5OZqDZZW6Bam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vejamos o que tem aqui...\n",
        "print(train_data)\n",
        "\n",
        "print(train_data[:10])\n",
        "\n",
        "print(train_data[-10:])"
      ],
      "metadata": {
        "id": "kODVhpi-ppnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c489d4d-22b6-42dd-e156-4e347290e910"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 8530\n",
            "})\n",
            "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'effective but too-tepid biopic', 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .', \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\", 'the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .', 'offers that rare combination of entertainment and education .', 'perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .', \"steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\", 'take care of my cat offers a refreshingly different slice of asian cinema .'], 'label': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "{'text': ['the star who helped give a spark to \" chasing amy \" and \" changing lanes \" falls flat as thinking man cia agent jack ryan in this summer\\'s new action film , \" the sum of all fears . \"', \"a summary of the plot doesn't quite do justice to the awfulness of the movie , for that comes through all too painfully in the execution .\", 'every conceivable mistake a director could make in filming opera has been perpetrated here .', \"snoots will no doubt rally to its cause , trotting out threadbare standbys like 'masterpiece' and 'triumph' and all that malarkey , but rarely does an established filmmaker so ardently waste viewers' time with a gobbler like this .\", '[the film\\'s] taste for \" shock humor \" will wear thin on all but those weaned on the comedy of tom green and the farrelly brothers .', 'any enjoyment will be hinge from a personal threshold of watching sad but endearing characters do extremely unconventional things .', \"if legendary shlockmeister ed wood had ever made a movie about a vampire , it probably would look a lot like this alarming production , adapted from anne rice's novel the vampire chronicles .\", \"hardly a nuanced portrait of a young woman's breakdown , the film nevertheless works up a few scares .\", 'interminably bleak , to say nothing of boring .', 'things really get weird , though not particularly scary : the movie is all portent and no content .'], 'label': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregando o modelo BERT\n",
        "\n",
        "Utilizaremos checkpoint \"bert-base-uncased\".\n",
        "\n",
        "Carregaremos o tokenizer, conforme exerc√≠cios anteriores, mas tamb√©m a classe `AutoModelForSequenceClassification`.\n",
        "\n",
        "A classe `AutoModelForSequenceClassification` da biblioteca Hugging Face Transformers √© uma classe autom√°tica projetada para simplificar o carregamento de modelos de Sequence Classification (Classifica√ß√£o de Sequ√™ncia) j√° criando nossa Task Head.\n",
        "\n",
        "Vejamos a diferen√ßa entre o carregamento do modelo original e o modelo com a Task Head Classification."
      ],
      "metadata": {
        "id": "oEsGaF3O6Cm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoModelForSequenceClassification, AutoConfig\n",
        "import torch.nn as nn\n",
        "\n",
        "# Suprime warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# O checkpoint base do BERT (sem ajuste fino para tarefa espec√≠fica)\n",
        "MODEL_CHECKPOINT = \"bert-base-uncased\"\n",
        "QNT_CLASSES = 2  # Definimos 2 classes (ex: positivo, negativo)\n",
        "\n",
        "print(\"--- 1. Carregando APENAS o Backbone BERT (BertModel) ---\")\n",
        "# AutoModel carrega o modelo base (o 'backbone' ou 'corpo' do transformer)\n",
        "modelo_base = AutoModel.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# Exibe a estrutura do modelo base\n",
        "print(modelo_base)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "print(f\"--- 2. Carregando BERT com a Cabe√ßa de Classifica√ß√£o ({QNT_CLASSES} classes) ---\")\n",
        "\n",
        "# 2.1. Criar uma configura√ß√£o para 2 classes\n",
        "config = AutoConfig.from_pretrained(MODEL_CHECKPOINT, num_labels=QNT_CLASSES)\n",
        "\n",
        "# 2.2. Carregar o modelo usando AutoModelForSequenceClassification\n",
        "modelo_class = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# Exibe a estrutura do modelo de Classifica√ß√£o\n",
        "print(modelo_class)"
      ],
      "metadata": {
        "id": "TGrpxX_ohyAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando novamente o modelo para utiliza√ß√£o em nossa pr√°tica"
      ],
      "metadata": {
        "id": "p9xHaCz9ryXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "modelo = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
      ],
      "metadata": {
        "id": "4AUBQ0rfaeuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí≠ Tokenizando a entrada"
      ],
      "metadata": {
        "id": "ZN86ciIPafax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# Necess√°rio para otimizar o padding no batch\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Definindo fun√ß√£o de preprocessamento dos dados\n",
        "def preprocessamento(itens):\n",
        "   return tokenizer(itens[\"text\"], truncation=True)\n",
        "\n",
        "# Tokenizando dados train e test\n",
        "tokenized_train = train_data.map(preprocessamento, batched=True)\n",
        "tokenized_test = test_data.map(preprocessamento, batched=True)"
      ],
      "metadata": {
        "id": "-RTfo1glfqFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defini√ß√£o de m√©tricas"
      ],
      "metadata": {
        "id": "LVFBZrjMfqip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Calculate F1 score\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    load_f1 = evaluate.load(\"f1\")\n",
        "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "    return {\"f1\": f1}"
      ],
      "metadata": {
        "id": "01YCr_HbfsF7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento"
      ],
      "metadata": {
        "id": "42LzS7V8f_dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Argumentos do treinamento\n",
        "training_args = TrainingArguments(\n",
        "   \"model\",\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=16,\n",
        "   per_device_eval_batch_size=16,\n",
        "   num_train_epochs=1,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        "   report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Instanciando o objeto \"treinador\"\n",
        "treinador = Trainer(\n",
        "   model=modelo,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "8dLXjLSsf_zI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treinador.train()"
      ],
      "metadata": {
        "id": "x7KEcV4_kzpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliando os resultados obtidos"
      ],
      "metadata": {
        "id": "9i0zYOdWgJy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "treinador.evaluate()"
      ],
      "metadata": {
        "id": "u-vfmXvogKF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ...mas o que foi treinado afinal de contas?\n",
        "\n",
        "Vamos exibir a configura√ß√£o dos par√¢metros para entender o que est√° acontecendo."
      ],
      "metadata": {
        "id": "MDZEaJF3uLOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibindo detalhes das camadas\n",
        "for name, param in modelo.named_parameters():\n",
        "    print(f\"Parameter: {name} ----- {param.requires_grad}\")"
      ],
      "metadata": {
        "id": "DTmXhL_qGL7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚õÑ ü•∂ üßä‚ùÑ Congelamento de Camadas üßäüßäüßä ‚òÉ ‚ùÑ ü•∂\n",
        "\n",
        "Uma t√°tica muito comum para fine tuning √© o congelamento de camadas, ou seja, bloquear a atualiza√ß√£o dos pesos no treinamento da rede neural.\n",
        "\n",
        "O üêçPyTorchüî• permite configurar explicitamente quais par√¢metros podem ser atualizados. O atributo `requires_grad` √© um boolean que controla se o par√¢metro requer c√°lculo de gradiente. Ou seja, quando `requires_grad == False` o sistema de autograd do PyTorch n√£o rastreia as opera√ß√µes que o envolvem, n√£o calcula nem armazena seu gradiente durante o backpropagation, e, consequentemente, o otimizador n√£o ajusta seu valor, mantendo-o constante ao longo de todo o processo de treinamento.\n",
        "\n",
        "---\n",
        "\n",
        "Vamos carregar novamente o modelo, mas agora vamos ajustar essa flag apenas nos par√¢metros da camada de classifica√ß√£o.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gevk9yXSgx4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model and Tokenizer\n",
        "modelo_bert_congelado = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "for name, param in modelo_bert_congelado.named_parameters():\n",
        "    # Ajusta a camada \"classifier\"\n",
        "    if name.startswith(\"classifier\"):\n",
        "      param.requires_grad = True\n",
        "    else:\n",
        "      param.requires_grad = False\n",
        "    print(f\"Parameter: {name} ----- {param.requires_grad}\")"
      ],
      "metadata": {
        "id": "PySQO8z9GMyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando o modelo carregado novamente\n",
        "\n",
        "Agora ser√° executado o processo de treinamento e vejamos a diferen√ßa."
      ],
      "metadata": {
        "id": "NiQ6G8suGMap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "treinador = Trainer(\n",
        "   model=modelo_bert_congelado,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "treinador.train()"
      ],
      "metadata": {
        "id": "2OnIna8_8Cqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nova Avalia√ß√£o do treinamento üèã"
      ],
      "metadata": {
        "id": "InGCasni8pPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "treinador.evaluate()"
      ],
      "metadata": {
        "id": "iBlR5GGo88-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5827afd1"
      },
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "# Pegar exemplos do dataset de teste (mistura de positivos e negativos)\n",
        "positive_reviews = test_data.filter(lambda example: example['label'] == 1).select(range(5))\n",
        "negative_reviews = test_data.filter(lambda example: example['label'] == 0).select(range(5))\n",
        "\n",
        "# Combinar os exemplos\n",
        "sample_reviews = concatenate_datasets([positive_reviews, negative_reviews])\n",
        "\n",
        "\n",
        "# Preprocessar as avalia√ß√µes individualmente\n",
        "tokenized_sample_list = [tokenizer(review, truncation=True) for review in sample_reviews['text']]\n",
        "\n",
        "# Usar o data collator para preparar a entrada para o modelo\n",
        "import torch\n",
        "batch = data_collator(tokenized_sample_list)\n",
        "\n",
        "# Mover o batch para o mesmo dispositivo do modelo\n",
        "batch = {k: v.to(modelo_bert_congelado.device) for k, v in batch.items()}\n",
        "\n",
        "\n",
        "# Fazer previs√µes\n",
        "with torch.no_grad():\n",
        "    outputs = modelo_bert_congelado(**batch)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "# Exibir os resultados\n",
        "for i, review in enumerate(sample_reviews['text']):\n",
        "    predicted_label = \"Positive\" if predictions[i].item() == 1 else \"Negative\"\n",
        "    # label real para fins de compara√ß√£o\n",
        "    actual_label = \"Positive\" if sample_reviews['label'][i] == 1 else \"Negative\"\n",
        "    print(f\"Review: {review}\\nPredicted Label: {predicted_label}\\nActual Label: {actual_label}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Para testar o classificador com uma string sua, utilize o c√≥digo abaixo:\n",
        "\n",
        "Lembre-se de enviar a vari√°vel para a GPU, para n√£o ver nenhum erro bizarro üòÖ."
      ],
      "metadata": {
        "id": "jiX8zI-IqzeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_teste = \"This is a horrible movie\"\n",
        "\n",
        "# Executa infer√™ncia no modelo treinado\n",
        "# Mudando para o modo eval\n",
        "modelo.eval()\n",
        "\n",
        "entrada_tokenizada = tokenizer(texto_teste, return_tensors=\"pt\")\n",
        "\n",
        "# Move os tensores para o dispositivo em que o modelo se encontra\n",
        "device = modelo.device\n",
        "entrada_tokenizada = {k: v.to(device) for k, v in entrada_tokenizada.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "  saida = modelo_bert_congelado(**entrada_tokenizada)\n",
        "\n",
        "print(f\"Sa√≠da bruta: {saida}\")\n",
        "\n",
        "# Trabalhando a sa√≠da bruta\n",
        "logits = saida.logits\n",
        "\n",
        "prob = torch.softmax(logits, dim=1)\n",
        "print(f\"Probabilidades: {prob}\")"
      ],
      "metadata": {
        "id": "wO-rX4TZrAdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b594d4cd-38bd-4460-8a62-1e8c580a9b29"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sa√≠da bruta: SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2159, -0.0121]], device='cuda:0'), hidden_states=None, attentions=None)\n",
            "Probabilidades: tensor([[0.5568, 0.4432]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Tarefas do Exerc√≠cio\n",
        "## 1. Agora responda com suas palavras o que aconteceu em ambos os casos durante o treinamento, evidenciando se voc√™ percebeu alguma diferen√ßa durante o passo de treinamento. Discuta brevemente os resultados obtidos."
      ],
      "metadata": {
        "id": "jAi-YJhT9COa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No segundo caso (modelo congelado), o treinamento √© mais leve e est√°vel, mas limitado, sendo eficiente quando o conjunto de dados √© pequeno\n",
        "# ou quando a tarefa √© semelhante ao que o BERT j√° sabe. No primeiro (modelo descongelado), o modelo aprende mais profundamente, o que normalmente\n",
        "# leva a desempenho superior, por√©m com maior custo de tempo e complexidade de treinamento."
      ],
      "metadata": {
        "id": "OmGcPqWj-KJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Conforme instru√ß√µes anteriores, carregue mais uma vez o modelo BERT, mas agora mantenha apenas a partir da camada encoder 10 (`bert.encoder.layer.10`) do modelo BERT como trein√°vel (`require_grad == True`). Ou seja, congele (`requires_grad == False`) at√© a camada encoder 9 (`bert.encoder.layer.9`)."
      ],
      "metadata": {
        "id": "pyBCsKgx-Rfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model and Tokenizer\n",
        "modelo_bert_congelado2 = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# 1) Congelar TODAS as camadas inicialmente\n",
        "for param in modelo_bert_congelado2.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2) Descongelar a partir da camada 10 (10 e 11)\n",
        "for param in modelo_bert_congelado2.bert.encoder.layer[10:].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# (Opcional) tamb√©m descongelar o pooler\n",
        "for param in modelo_bert_congelado2.bert.pooler.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# O classificador final j√° vem descongelado por padr√£o\n",
        "for param in modelo_bert_congelado2.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "#testa se deu certo\n",
        "for name, param in modelo_bert_congelado2.named_parameters():\n",
        "  if \"encoder.layer.9\" in name:\n",
        "    print(name, \"->\", param.requires_grad)\n",
        "  if \"encoder.layer.10\" in name:\n",
        "    print(name, \"->\", param.requires_grad)"
      ],
      "metadata": {
        "id": "g2uLu1xb-SOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Execute o treinamento com os mesmos par√¢metros utilizados anteriormente e execute o m√©todo `evaluate()` para calcular as m√©tricas e discuta os resultados obtidos.\n",
        "Caso julgar necess√°rio, execute outros treinamentos ajustando quantidades distintas de par√¢metros trein√°veis para tirar conclus√µes adicionais."
      ],
      "metadata": {
        "id": "eU2EwUJhDnyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "treinador = Trainer(\n",
        "   model=modelo_bert_congelado2,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "treinador.train()"
      ],
      "metadata": {
        "id": "OmJ0zHbTD56S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treinador.evaluate()"
      ],
      "metadata": {
        "id": "F60lHzIMdhWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. (B√îNUS) Pesquise e escolha outro dataset para fazer fine tuning do modelo BERT."
      ],
      "metadata": {
        "id": "U2Z0zXA49fcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#Dataset SST-2 (Stanford Sentiment Treebank 2)\n",
        "#Tarefa: Sentiment Analysis (positivo/negativo)\n",
        "#Tamanho: ~70k exemplos\n",
        "#Fonte: GLUE Benchmark\n",
        "#Dom√≠nio: Frases de reviews de filmes\n",
        "\n",
        "sst2 = load_dataset(\"glue\", \"sst2\")\n",
        "ds_train = sst2[\"train\"]        # ~67k\n",
        "ds_valid = sst2[\"validation\"]   # ~872\n",
        "ds_test  = sst2[\"test\"]         # sem labels\n",
        "\n",
        "len(ds_train), len(ds_valid), len(ds_test)\n"
      ],
      "metadata": {
        "id": "1iIAfkwq9fw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "pretrained = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizerFast.from_pretrained(pretrained)\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"sentence\"],\n",
        "        truncation=True,\n",
        "        padding=False,   # padding din√¢mico via DataCollator\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "ds_train_tok = ds_train.map(tokenize, batched=True, remove_columns=[\"sentence\", \"idx\"])\n",
        "ds_valid_tok = ds_valid.map(tokenize, batched=True, remove_columns=[\"sentence\", \"idx\"])\n",
        "ds_test_tok  = ds_test.map(tokenize,  batched=True, remove_columns=[\"sentence\", \"idx\"])\n",
        "\n",
        "ds_train_tok = ds_train_tok.rename_column(\"label\", \"labels\")\n",
        "ds_valid_tok = ds_valid_tok.rename_column(\"label\", \"labels\")\n",
        "# ds_test n√£o tem \"label\"\n",
        "\n",
        "ds_train_tok.set_format(\"torch\")\n",
        "ds_valid_tok.set_format(\"torch\")\n",
        "ds_test_tok.set_format(\"torch\")\n"
      ],
      "metadata": {
        "id": "G-hQjKJeg_VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model and Tokenizer\n",
        "modelo_bert_congelado3 = AutoModelForSequenceClassification.from_pretrained(MODEL_CHECKPOINT, num_labels=2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# 1) Congelar TODAS as camadas inicialmente\n",
        "for param in modelo_bert_congelado3.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2) Descongelar a partir da camada 10 (10 e 11)\n",
        "for param in modelo_bert_congelado3.bert.encoder.layer[10:].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# (Opcional) tamb√©m descongelar o pooler\n",
        "for param in modelo_bert_congelado3.bert.pooler.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# O classificador final j√° vem descongelado por padr√£o\n",
        "for param in modelo_bert_congelado3.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "#testa se deu certo\n",
        "for name, param in modelo_bert_congelado3.named_parameters():\n",
        "  if \"encoder.layer.9\" in name:\n",
        "    print(name, \"->\", param.requires_grad)\n",
        "  if \"encoder.layer.10\" in name:\n",
        "    print(name, \"->\", param.requires_grad)"
      ],
      "metadata": {
        "id": "BDffnZivebbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "treinador3 = Trainer(\n",
        "   model=modelo_bert_congelado3,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train,\n",
        "   eval_dataset=tokenized_test,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "treinador.train()"
      ],
      "metadata": {
        "id": "mRAiJJawebIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treinador3.evaluate()"
      ],
      "metadata": {
        "id": "IH2lqLe0gISy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}