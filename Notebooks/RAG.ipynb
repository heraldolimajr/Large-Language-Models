{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyDUdr7OR5in"
      },
      "source": [
        "# Parte 1 - RAG - Retrieval-Augmented Generation\n",
        "\n",
        "### Objetivo: Desenvolver um sistema que responde perguntas sobre um conjunto de artigos científicos locais (PDFs), usando uma abordagem de Retrieval-Augmented Generation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_PE3RS8H-Vy"
      },
      "source": [
        "#ATIVIDADE\n",
        "\n",
        "- Altere os pontos marcados com #TODO(tópico 5 e 6)\n",
        "- Carregar seus próprios artigos e datasets em PDF (tópico 2).\n",
        "- Usar modelo gratuito (nossa sugestão é o llama via groq).\n",
        "- Avaliar respostas automaticamente com métricas de NLP.\n",
        "\n",
        "**Observação 01:** cada aluno deve adaptar o código a um domínio específico da sua linha de pesquisa (ex: Engenharia de software, IHC, IA, robótica, etc) e comparar a performance.\n",
        "\n",
        "\n",
        "**Observação 02:** Caso necessário, faça suas alterações no código, conforme os conceitos vistos em sala de aula, para adequar ao caso específico que esteja tratando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iTUawt-is_2",
        "outputId": "69370277-1f3a-4d70-9840-da5c6ceb591f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQony8PvbxrD"
      },
      "source": [
        "# 1. Upload dos PDFs\n",
        "Você carregará os PDFs que gostaria que fossem analisados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "1rkt0m6QhVYa",
        "outputId": "aed1b7d8-9794-400e-af20-82e9d66007f0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9b6d73ec-cbec-4c1d-b667-2c081b73c281\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9b6d73ec-cbec-4c1d-b667-2c081b73c281\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Book-Web-Semantica.pdf to Book-Web-Semantica.pdf\n",
            "PDFs carregados: ['Book-Web-Semantica.pdf']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Cria pasta para os PDFs\n",
        "os.makedirs(\"corpus\", exist_ok=True)\n",
        "for fname in uploaded.keys():\n",
        "    os.rename(fname, os.path.join(\"corpus\", fname))\n",
        "\n",
        "print(\"PDFs carregados:\", os.listdir(\"corpus\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beI2Uv0nb-Vq"
      },
      "source": [
        "# 2. Leitura e extração do texto dos PDFs\n",
        "A função abaixo irá gerar o corpus (que é uma lista de textos). Cada elemento do corpus é o texto de um PDF carregado anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBytcW0LQaUI",
        "outputId": "42e8beb7-87e2-42f6-c216-6c8121fefeae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "133 chunks carregados.\n"
          ]
        }
      ],
      "source": [
        "def load_papers(folder):\n",
        "    corpus = []\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith(\".pdf\"):\n",
        "            reader = PdfReader(os.path.join(folder, file))\n",
        "            text = \" \"\n",
        "            for page in reader.pages:\n",
        "                text = page.extract_text() or \" \"\n",
        "                corpus.append(text)\n",
        "    return corpus\n",
        "\n",
        "texts = load_papers(\"corpus\")\n",
        "print(f\"{len(texts)} chunks carregados.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJVxwRnoRRzo"
      },
      "source": [
        "# 3. Embeddings - Criação\n",
        "Usar o sentence-transformers para transformar os textos extraídos dos PDFs em embeddings. (Se colab pedir acesso, conceda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9eyvSI6RSsB"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(texts, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrIZp-buRUpf"
      },
      "source": [
        "# 4. Função de Recuperação - (R)AG\n",
        "### Implementar o mecanismo de busca vetorial. Aqui entra o retriever: busca semântica por similaridade de embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fem0-I7-Rqvm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def retrieve(query, texts, embeddings, top_k=2, max_chars=3000):\n",
        "    \"\"\"\n",
        "    Recupera os textos mais relevantes limitando o tamanho total (max_chars)\n",
        "    para não exceder o limite de tokens do modelo Groq.\n",
        "    \"\"\"\n",
        "    query_emb = model.encode([query])\n",
        "    scores = cosine_similarity(query_emb, embeddings)[0]\n",
        "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    total_len = 0\n",
        "    for i in top_indices:\n",
        "        snippet = texts[i]\n",
        "        if total_len + len(snippet) > max_chars:\n",
        "            snippet = snippet[: max_chars - total_len]  # corta para caber no limite\n",
        "        results.append(snippet)\n",
        "        total_len += len(snippet)\n",
        "        if total_len >= max_chars:\n",
        "            break\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUHb9GIXSZxz"
      },
      "source": [
        "# 5. Integrar com uma LLM - R(AG)\n",
        "\n",
        "### O conteúdo recuperado é passado como contexto ao modelo llama-3.3:70b a partir do groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oqZpUFoajhwc",
        "outputId": "10a995ea-3362-4c33-c3da-37857622540e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.0.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.79)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-1.0.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-1.0.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
            "  Downloading langgraph-1.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.38)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Collecting groq<1.0.0,>=0.30.0 (from langchain-groq)\n",
            "  Downloading groq-0.33.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_prebuilt-1.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq<1.0.0,>=0.30.0->langchain-groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n",
            "Downloading langchain-1.0.3-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.3-py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.9/469.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-1.0.0-py3-none-any.whl (16 kB)\n",
            "Downloading groq-0.33.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-1.0.2-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.2-py3-none-any.whl (34 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, groq, langchain-core, langgraph-checkpoint, langchain-groq, langgraph-prebuilt, langgraph, langchain\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "Successfully installed groq-0.33.0 langchain-1.0.3 langchain-core-1.0.3 langchain-groq-1.0.0 langgraph-1.0.2 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.2 langgraph-sdk-0.2.9 ormsgpack-1.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain langchain-core langchain-groq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZWbkML1Sjdr",
        "outputId": "3b3979b5-df6f-4182-f9cd-d2fc26a7b115"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "#TODO sigas os passos em https://groq.com/ para pegar sua chave: Developers -> Free API key\n",
        "GROQ_API_KEY = input( \"Cole sua chave Groq aqui e dê <enter>: \")\n",
        "\n",
        "client = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    api_key=GROQ_API_KEY,\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "def generate_answer(query, context):\n",
        "    prompt = f\"\"\"\n",
        "Use o contexto abaixo para responder a pergunta com precisão científica.\n",
        "Contexto: {context}\n",
        "Pergunta: {query}\n",
        "\"\"\"\n",
        "    response = client.invoke(prompt).content\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzuKGhnGdqAb"
      },
      "source": [
        "# 6. Teste do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTqtef1shJNJ",
        "outputId": "bff4ea7d-a35c-485c-e8b6-690630d383c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Resposta gerada:\n",
            " Ontologias são modelos conceituais que definem um conjunto de conceitos, relações e regras para representar um domínio específico de conhecimento. Elas são utilizadas para fornecer uma estrutura comum para a representação e compartilhamento de informações, permitindo que diferentes sistemas e aplicativos possam entender e interagir com essas informações de forma consistente.\n",
            "\n",
            "No contexto da Web Semântica, as ontologias são utilizadas para definir a estrutura e o significado dos dados, permitindo que os computadores possam entender e processar esses dados de forma mais eficaz. As ontologias podem ser utilizadas para representar conceitos como entidades, relações, propriedades e restrições, e podem ser utilizadas em uma variedade de aplicações, incluindo busca de informações, integração de dados e tomada de decisões.\n",
            "\n",
            "No exemplo mencionado no texto, a ontologia PROV é utilizada para representar a proveniência de dados, ou seja, a origem e a história dos dados. Ela define um modelo que inclui entidades, atividades e agentes, e relações entre eles, permitindo que os dados sejam representados de forma mais precisa e consistente.\n",
            "\n",
            "Em resumo, as ontologias são modelos conceituais que fornecem uma estrutura comum para a representação e compartilhamento de informações, permitindo que os computadores possam entender e processar essas informações de forma mais eficaz. Elas são uma ferramenta importante na Web Semântica e em outras aplicações que envolvem a representação e processamento de dados.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "query = \"O que são ontologias?\"\n",
        "context = \" \".join(retrieve(query, texts, embeddings))\n",
        "answer = generate_answer(query, context)\n",
        "print(\"\\nResposta gerada:\\n\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ5B6NJgpv6q"
      },
      "source": [
        "# Parte 2 - Pesquisa na web - Web-based RAG ou Online RAG\n",
        "Nesta seção, faremos uma prática de buscas de informações na web. O objetivo é dar ao modelo dados atualizados retirados de artigos na web. Neste exemplo, no Retrieval **(Recuperação)**, ao invés de buscarmos de um corpus de documentos ou banco de dados, buscaremos da web. O conteúdo extraído da página da web será usado para Aumentar **(Augment)** o prompt fornecido ao modelo. E por fim, o modelo usará o prompt para Gerar **(Generation)** uma resposta que será o resumo de um artigo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a56fb4b7"
      },
      "source": [
        "Em resumo, iremos:\n",
        "demonstrar um fluxo de RAG (Retrieval Augmented Generation) buscando informações na internet usando DuckDuckGo, extraindo o conteúdo de um artigo relevante e resumindo-o usando o LLM pelo Groq."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJP8QFLTG5gK"
      },
      "source": [
        "#ATIVIDADE\n",
        "\n",
        "**Altere os pontos marcados com #TODO** no código para testar diferentes\n",
        "resultados.\n",
        "\n",
        "Por exemplo, se houver: QUANT_MAX_ARTIGOS = 5  # TODO\n",
        "mude para 10, por exemplo e veja como muda a seleção de artigos.\n",
        "\n",
        "**Escolha o artigo que será usado.**\n",
        "\n",
        "Por padrão, search_results[0] pega apenas o primeiro. Você pode testar com outro índice para ver respostas diferentes.\n",
        "\n",
        "**Volte à Parte 1 e repita a execução.**\n",
        "\n",
        "Lá o código junta os resultados do retrieve() nos chunks e passa esse contexto para o LLM.\n",
        "\n",
        "Assim, você consegue comparar como as alterações influenciam a resposta final do modelo.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dedb7401"
      },
      "source": [
        "## 1. Instalar bibliotecas necessárias\n",
        "\n",
        "Instalar bibliotecas para buscar na web (DuckDuckGo) e para extrair o conteúdo de páginas web.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24ffc119",
        "outputId": "bc34dcd8-2e8d-491e-ca11-fb644820d0ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ddgs\n",
            "  Downloading ddgs-9.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.3.0)\n",
            "Collecting primp>=0.15.0 (from ddgs)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting lxml>=6.0.0 (from ddgs)\n",
            "  Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
            "Collecting socksio==1.* (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.15.0)\n",
            "Downloading ddgs-9.7.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: socksio, primp, lxml, ddgs\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.4.0\n",
            "    Uninstalling lxml-5.4.0:\n",
            "      Successfully uninstalled lxml-5.4.0\n",
            "Successfully installed ddgs-9.7.1 lxml-6.0.2 primp-0.15.0 socksio-1.0.0\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install ddgs\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77e4c064"
      },
      "source": [
        "##2. Realizar busca na web\n",
        "\n",
        "Usar a ferramenta de busca para encontrar artigos relevantes com base em uma consulta do usuário. O duckduckgo (ddgs) faz o trabalho de buscar artigos na Internet, assim como o Google.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bfbf6fe",
        "outputId": "3ac956cf-3932-4b55-d6f7-98de8ddd4375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultados da busca:\n",
            "Título: OWL Web Ontology Language Semantics and Abstract Syntax\n",
            "URL: https://www.w3.org/TR/owl-semantics/\n",
            "Descrição: 30 May 2003] Per a decision of the Web Ontology working group on 29 May 2003 to change the semantics for owl:intersectionOf and related resources ...\n",
            "\n",
            "Título: Web Ontology Language (OWL) Abstract Syntax and Semantics\n",
            "URL: https://www.w3.org/TR/2002/WD-owl-semantics-20021108/\n",
            "Descrição: The n-triples syntax and RDFS-compatible semantics allows annotation of ontologies by using RDF properties on the document URI and annotation of ...\n",
            "\n",
            "Título: Semantic Web Programming and Ontology Construction Courses\n",
            "URL: https://ftt.co.uk/Semantic_Web_Courses.php\n",
            "Descrição: Semantic Web Programming and Ontology Construction Courses ... SWEB101 Introduction to the Semantic Web - RDF, RDFS, OWL and Ontologies (5 days)\n",
            "\n",
            "Título: Ontology and the Semantic Web PDF Download Free | 1586037293\n",
            "URL: https://ebooks-it.org/1586037293-ebook.htm\n",
            "Descrição: ... Semantic Web works technically and how businesses can utilize it to gain a competitive advantage* Explains what taxonomies and ontologies are as well ...\n",
            "\n",
            "Título: Ontology Management Semantic Web, Semantic Web Services, and\n",
            "URL: https://ebooks-it.org/038769899x-ebook.htm\n",
            "Descrição: Ontology and the Semantic Web ... Semantic Web in the late 1990s, they have been studied as a kind of software artifact in their own right, called an ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from ddgs import DDGS\n",
        "\n",
        "ddgs = DDGS()\n",
        "QUANT_MAX_ARTIGOS = 5\n",
        "\n",
        "query = \"Ontology and Semantic Web\"\n",
        "search_results = ddgs.text(query, max_results=QUANT_MAX_ARTIGOS)\n",
        "\n",
        "print(\"Resultados da busca:\")\n",
        "for result in search_results:\n",
        "    print(f\"Título: {result['title']}\")\n",
        "    print(f\"URL: {result['href']}\")\n",
        "    print(f\"Descrição: {result['body']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "078ad542"
      },
      "source": [
        "##3. Extrair conteúdo do artigo\n",
        "\n",
        "Acessar a URL do artigo retornado pela busca e extrair o texto principal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7767701",
        "outputId": "279444b8-8c1f-4758-d467-6746e9c7594f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conteúdo do artigo extraído da URL: https://ftt.co.uk/Semantic_Web_Courses.php\n",
            "Primeiros 500 caracteres do texto extraído:\n",
            "Standard and Advanced Technical Training, Consultancy and Mentoring\n",
            "\n",
            " Semantic Web Programming and Ontology Construction Courses  \n",
            "\r\n",
            "At present, information on the web is oriented towards human consumption. \r\n",
            "Computers are presently mainly used as means for storing and conveying that information. \r\n",
            "Humans understand the meaning of the information they are using, \r\n",
            "whereas, for a computer that information is just a large collection of symbols. \r\n",
            "In order for computers to be able to use web inform\n",
            "1993\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "article_text = None\n",
        "if not search_results:\n",
        "    print(\"Nenhuma URL encontrada para extração.\")\n",
        "else:\n",
        "\n",
        "    #Escolher um artigo aleatoriamente\n",
        "    indices = list(range(QUANT_MAX_ARTIGOS))\n",
        "    #Embaralha a ordem dos índices\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    #Tenta acessar os artigos em ordem aleatória até encontrar um que possa ser baixado com sucesso.\n",
        "    for idx in indices:\n",
        "        article_url = search_results[idx]['href']\n",
        "\n",
        "\n",
        "    try:\n",
        "        response = requests.get(article_url, timeout=10) # Adicionado timeout para evitar travamentos\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            # Tentativa de encontrar o texto principal. Isso pode precisar de ajustes\n",
        "            # dependendo da estrutura HTML dos sites.\n",
        "            article_text = \"\"\n",
        "            paragraphs = soup.find_all('p')\n",
        "            for p in paragraphs:\n",
        "                article_text += p.get_text() + \"\\n\"\n",
        "\n",
        "            if article_text:\n",
        "                print(f\"Conteúdo do artigo extraído da URL: {article_url}\")\n",
        "                print(\"Primeiros 500 caracteres do texto extraído:\")\n",
        "                print(article_text[:500])\n",
        "            else:\n",
        "                print(f\"Não foi possível extrair texto principal da URL: {article_url}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Erro ao acessar a URL {article_url}. Código de status: {response.status_code}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erro ao acessar a URL {article_url}: {e}\")\n",
        "\n",
        "if article_text:\n",
        "    print(len(article_text))\n",
        "else:\n",
        "    #Imprime esta mensagem caso dê erro na extração\n",
        "    print(\"Nenhum artigo extraído.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9143623"
      },
      "source": [
        "##4. Quebrando os chunks\n",
        "\n",
        "Quebrar em chunks o texto extraído para utilizar com contexto. Utilizando outra abordagem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOsJ7ZnAV-_Q",
        "outputId": "ba79e9dc-6693-4e39-ac94-5f558654fce8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (0.3.11)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (0.4.38)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c48d5b2",
        "outputId": "9f54e960-02a4-40ce-a842-b70a80c5c89d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número total de Chunks criados: **2**\n",
            "\n",
            "--------------------------------------------------\n",
            "*** CHUNK 1 (Tamanho: 1226 caracteres) ***\n",
            "Standard and Advanced Technical Training, Consultancy and Mentoring\n",
            "\n",
            " Semantic Web Programming and Ontology Construction Courses  \n",
            "\r\n",
            "At present, information on the web is oriented towards human consumption. \r\n",
            "Computers are presently mainly used as means for storing and conveying that information. \r\n",
            "Humans understand the meaning of the information they are using, \r\n",
            "whereas, for a computer that information is just a large collection of symbols. \r\n",
            "In order for computers to be able to use web information they need some mechanism \r\n",
            "for understanding its meaning. \r\n",
            "This is the classical issue of information vs. knowledge. \r\n",
            "Knowledge permits reasoning and planning. \r\n",
            "As the web is a network oriented medium so, knowledge on the web can be \r\n",
            "envisaged as a net - in effect, a semantic network. \r\n",
            "\n",
            "\r\n",
            "Web pages use markup in the form of HTML/XHTML. XHTML documents are based on XML, \r\n",
            "XML itself has no mechanisms for representing semantic information. \r\n",
            "After almost two decades of research there are now stable and robust technologies \r\n",
            "for building semantic web applications. \r\n",
            "The key technologies are \r\n",
            "\n",
            " RDF (Resource Description Framework)\n",
            " RDFS (RDF Schema) \n",
            " OWL (Web Ontology Language)\n",
            "\n",
            "\r\n",
            "These are all based on XML.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 2 (Tamanho: 878 caracteres) ***\n",
            "RDF (Resource Description Framework)\n",
            " RDFS (RDF Schema) \n",
            " OWL (Web Ontology Language)\n",
            "\n",
            "\r\n",
            "These are all based on XML. \r\n",
            "\n",
            "\r\n",
            "The RDF approach is quite complex. For classical web sites and applications \r\n",
            "it would be much easier to provide semantic information via properties in HTML \r\n",
            "tags that contain semantic information. This, more recent initiative, has resulted \r\n",
            "in the development of RDFa, which is, in effect, a micro-annotation mechanism. \r\n",
            "RDFa is based on RDF. RDFa has great potential for organisations that need \r\n",
            "to provide easily accessible information such as local and central government departments \r\n",
            "and agencies, the NHS, etc..\r\n",
            "\n",
            "\n",
            "Semantic Web and RDFa Courses\n",
            "\n",
            "\r\n",
            "\tSWEB101 Introduction to the Semantic Web - RDF, RDFS, OWL and Ontologies (5 days) \n",
            "\n",
            "\r\n",
            "\tSWEB102 Introduction to RDFa - Concepts, e-Business, e-Government and Social Networking Applications (2 days)\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Configura o Text Splitter\n",
        "# Instancia o separador de texto com as suas especificações\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Tamanho máximo de cada chunk em caracteres\n",
        "    chunk_size=1500,\n",
        "    # Tamanho de sobreposição entre chunks.\n",
        "    # Isso ajuda a manter o contexto entre os chunks adjacentes.\n",
        "    chunk_overlap=250,\n",
        "    # Separadores que o splitter tentará usar, em ordem:\n",
        "    # 1. Parágrafos (\\n\\n)\n",
        "    # 2. Novas linhas (\\n)\n",
        "    # 3. Espaços (' ')\n",
        "    # 4. Caracteres vazios ('')\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    length_function=len # Função usada para medir o tamanho (len para caracteres)\n",
        ")\n",
        "\n",
        "# 3. Quebrar o texto\n",
        "chunks = text_splitter.create_documents([article_text])\n",
        "\n",
        "# 4. Imprimir os resultados para verificação\n",
        "\n",
        "print(f\"Número total de Chunks criados: **{len(chunks)}**\\n\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Itera sobre os chunks (objetos Document do LangChain)\n",
        "for i, chunk in enumerate(chunks):\n",
        "    content = chunk.page_content\n",
        "    print(f\"*** CHUNK {i+1} (Tamanho: {len(content)} caracteres) ***\")\n",
        "    print(content)\n",
        "    print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
