{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heraldolimajr/Large-Language-Models/blob/main/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Heraldo** Gon√ßalves Lima Junior\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DJAcpasko2Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exerc√≠cio de Self-Attention com PyTorch ü§ñ\n",
        "Neste exerc√≠cio, voc√™ ir√° codificar o mecanismo de Self-Attention do zero, com sugest√µes utilizando a biblioteca PyTorch.\n",
        "\n",
        "Conforme visto em sala, o Self-attention √© um componente central dos Transformers, as redes neurais que impulsionam os modelos de linguagem modernos como o BERT. Entender o self-attention √© fundamental para compreender como esses modelos conseguem processar e interpretar a linguagem.\n",
        "\n",
        "## Contexto\n",
        "Um Transformer processa um texto de entrada, que √© dividido em tokens. O Self-attention permite que o modelo determine a rela√ß√£o entre diferentes tokens em uma sequ√™ncia.\n",
        "\n"
      ],
      "metadata": {
        "id": "WwxUW7Wi0yTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dicas de ferramental\n",
        "\n",
        "Para implementar ser√° necess√°rio saber realizar algumas opera√ß√µes. Como por exemplo inicializar pesos, multiplica√ß√£o entre matrizes, calcular transposta etc. Vamos utilizar o framework `PyTorch` como sugest√£o, mas fica a cargo de cada um escolher a tecnologia que sinta mais √† vontade.\n",
        "\n",
        "\n",
        "## Multiplica√ß√£o de matrizes\n",
        "\n",
        "Podemos utilizar a fun√ß√£o `torch.matmul()`. A fun√ß√£o matmul pode ser utilizada para realizar a multiplica√ß√£o de matrizes entre **q** e **k**. O resultado esperado, √© uma matriz de pontua√ß√µes que indica a similaridade de cada query com cada key. Uma pontua√ß√£o alta significa que um token √© muito similar a outro.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Amx776yy-SpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de multiplica√ß√£o de matrizes\n",
        "import torch\n",
        "\n",
        "# Define as matrizes\n",
        "matriz_A = torch.tensor([[1, 2], [3, 4]])\n",
        "matriz_B = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "# Multiplica as matrizes\n",
        "torch.matmul(matriz_A, matriz_B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vok-6UctTwX",
        "outputId": "fc9f8a73-842d-4729-bf28-ad8d14c0d18e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[19, 22],\n",
              "        [43, 50]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transposta de uma matriz\n",
        "\n",
        "Podemos utilizar o m√©todo `.transpose()` para calcular a transposta, conforme exemplo abaixo a seguir. A fun√ß√£o do matriz.transpose(0, 1) inverte a primeira dimens√£o (√≠ndice 0, que s√£o as linhas) com a segunda dimens√£o (√≠ndice 1, que s√£o as colunas), resultando na matriz transposta.\n"
      ],
      "metadata": {
        "id": "wgG9dcdotUHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a matriz\n",
        "matriz = torch.tensor([[1, 2, 3],\n",
        "                       [4, 5, 6]])\n",
        "\n",
        "# Transp√µe a matriz\n",
        "matriz.transpose(0, 1)"
      ],
      "metadata": {
        "id": "1Dh15LaMABp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ddae12-99b1-4707-9740-9a13e7215b0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 4],\n",
              "        [2, 5],\n",
              "        [3, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Divis√£o de matrizes\n",
        "\n",
        "Para efetuar o escalonamento das similaridades, evitando que valores muito grandes dominem o processo de `softmax`, ser√° necess√°rio efetuar uma divis√£o por um valor escalar calculado, conforme a f√≥rmula vista na aula. Segue exemplo de como fazer com PyTorch abaixo:"
      ],
      "metadata": {
        "id": "EhzNjMOJt9il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a matriz\n",
        "matriz = torch.tensor([[10, 20], [30, 40]])\n",
        "escalar = 2\n",
        "\n",
        "# Divide a matriz por um escalar\n",
        "matriz_dividida = matriz / escalar\n",
        "\n",
        "matriz_dividida"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0lTaTISt9FW",
        "outputId": "a639bb10-dd01-4da4-ad65-0af02d68180b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5., 10.],\n",
              "        [15., 20.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplica√ß√£o de Softmax\n",
        "\n",
        "Ap√≥s o escalonamento, ser√° necess√°rio transformar em porcentagens de aten√ß√£o as pontua√ß√µes de similaridade obtidas. Para isso pode se utilizar a fun√ß√£o `softmax`, que converte uma lista de n√∫meros em uma distribui√ß√£o de probabilidade. Ela garante que todos os valores somem 1. Cada valor na matriz de percentuais calculada representa o peso de aten√ß√£o, ou seja, a porcentagem de \"import√¢ncia\" que um token deve dar a todos os outros tokens da sequ√™ncia.\n",
        "\n",
        "O par√¢metro `dim` diz ao PyTorch se essa convers√£o deve ser feita linha por linha, coluna por coluna ou ao longo de outra dimens√£o espec√≠fica. Para nosso caso deve utilizar a dimens√£o da coluna.\n",
        "\n",
        "```python\n",
        "perc_atencao = torch.nn.functional.softmax(s_similiridades, dim=self.col_dim)\n",
        "```"
      ],
      "metadata": {
        "id": "AwR0wrRO4i-T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m9VNpwvqtTZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Tarefas do Exerc√≠cio\n",
        "## 1. Codifique uma Classe B√°sica de Self-Attention ‚úçÔ∏è\n",
        "Sua primeira tarefa √© completar a classe SelfAttention. Esta classe receber√° as codifica√ß√µes de tokens como entrada e executar√° os c√°lculos de self-attention para produzir as pontua√ß√µes de aten√ß√£o. O m√©todo __init__ j√° est√° fornecido, mas voc√™ precisar√° preencher o m√©todo forward. Levando em considera√ß√£o a equa√ß√£o vista:\n",
        "\n",
        "\n",
        "\n",
        "$$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "\n",
        "\n",
        "**Instru√ß√µes**:\n",
        "\n",
        "* Voc√™ trabalhar√° no m√©todo `forward` da classe `SelfAttention`.\n",
        "\n",
        "* Crie os vetores de Queries, Keys e Values: Use as matrizes de pesos fornecidas (self.W_q, self.W_k e self.W_v) para transformar as token_encodings de entrada nos vetores de query (q), key (k) e value (v).\n",
        "\n",
        "* Calcule as Pontua√ß√µes de Similaridade: Compute as pontua√ß√µes de similaridade realizando a multiplica√ß√£o da matriz de queries (q) pela transposta das keys (k\n",
        "T\n",
        " ).\n",
        "\n",
        "* Dimensione as Pontua√ß√µes: Dimensione as pontua√ß√µes de similaridade dividindo-as pela raiz quadrada do n√∫mero de colunas nas keys.\n",
        "\n",
        "* Aplique o Softmax: Aplique a fun√ß√£o softmax √†s pontua√ß√µes dimensionadas para obter as porcentagens de aten√ß√£o. Lembre-se de especificar a dimens√£o sobre a qual o softmax deve ser aplicado.\n",
        "\n",
        "* Calcule as Pontua√ß√µes Finais de Aten√ß√£o: Multiplique as porcentagens de aten√ß√£o pelos values (v) para obter as pontua√ß√µes finais de aten√ß√£o.\n",
        "\n",
        "**Observa√ß√µes:**\n",
        "\n",
        "-  Por motivos did√°ticos, n√£o utilizaremos bias, nem faremos o treinamento dos pesos, nem trabalharemos com gera√ß√£o de embeddings. Portanto basta inicializar definindo um seed, conforme c√≥digo de exemplo, para que possamos verificar se os c√°lculos est√£o sendo feitos corretamente com os valores de embeddings sugeridos.\n"
      ],
      "metadata": {
        "id": "RwAGv2nZ1CJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HcIOibDO0xdD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Uma classe b√°sica de Self-Attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=2,\n",
        "                 row_dim=0,\n",
        "                 col_dim=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Inicializa os Pesos (W) para queries, keys e values\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "        self.row_dim = row_dim\n",
        "        self.col_dim = col_dim\n",
        "\n",
        "    def forward(self, token_encodings):\n",
        "        ## SEU C√ìDIGO AQUI!\n",
        "        ## Crie queries, keys e values.\n",
        "        q = self.W_q(token_encodings)\n",
        "        k = self.W_k(token_encodings)\n",
        "        v = self.W_v(token_encodings)\n",
        "\n",
        "        ## Calcule as pontua√ß√µes de similaridade.\n",
        "        #transposta de k\n",
        "        k_t = k.transpose(0,1)\n",
        "\n",
        "        ## Compute as pontua√ß√µes de similaridade.\n",
        "        sims = torch.matmul(q,k_t)\n",
        "\n",
        "        ## Dimensione as similaridades.\n",
        "        dim_k = k.size(self.col_dim)\n",
        "        scaled_sims = sims/ dim_k ** 0.5\n",
        "\n",
        "        ## Aplique softmax para obter as porcentagens de aten√ß√£o.\n",
        "        perc_atencao = torch.nn.functional.softmax(scaled_sims, dim=self.col_dim)\n",
        "\n",
        "        ## Calcule as pontua√ß√µes finais de aten√ß√£o.\n",
        "        attention_scores = torch.matmul(perc_atencao,v)\n",
        "\n",
        "        return attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 2. Teste sua Classe de Self-Attention üß™\n",
        "Ap√≥s completar a classe SelfAttention, voc√™ a testar√° com alguns dados de exemplo.\n",
        "\n",
        "**Instru√ß√µes:**\n",
        "\n",
        "Execute o c√≥digo abaixo.\n",
        "\n",
        "A sa√≠da do seu c√≥digo deve corresponder √† sa√≠da esperada."
      ],
      "metadata": {
        "id": "Nk_piNCA61is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crie uma matriz de codifica√ß√µes de token\n",
        "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
        "                                 [0.57, 1.36],\n",
        "                                 [4.41, -2.16]])\n",
        "\n",
        "# Defina a semente para reprodutibilidade\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Crie um objeto de self-attention\n",
        "selfAttention = SelfAttention()\n",
        "\n",
        "# Calcule as pontua√ß√µes de aten√ß√£o para as codifica√ß√µes de token\n",
        "selfAttention(encodings_matrix)"
      ],
      "metadata": {
        "id": "XUX2aYQV7EZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf8254b-3d59-44c7-b70e-6988ec5fb3e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0100, 1.0641],\n",
              "        [0.2040, 0.7057],\n",
              "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sa√≠da Esperada:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "tensor([[1.0100, 1.0641],\n",
        "        [0.2040, 0.7057],\n",
        "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jkGcx2L87IrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Implemente a Masked Self-Attention ‚úçÔ∏è\n",
        "Sua tarefa √© modificar a classe `SelfAttention` anterior para incluir o comportamento de m√°scara criar uma nova classe `MaskedSelfAttention`.\n",
        "\n",
        "## **Instru√ß√µes:**\n",
        "\n",
        "* A partir do c√≥digo da sua classe `SelfAttention`, adicione uma l√≥gica de mascaramento antes da fun√ß√£o softmax.\n",
        "\n",
        "* Voc√™ precisar√° criar uma m√°scara (mask) que tenha o mesmo formato da matriz de pontua√ß√µes de similaridade (`sims`). Esta m√°scara deve conter zeros na diagonal principal e abaixo dela, e `-inf` (n√∫mero muito grande e negativo) acima dela.\n",
        "\n",
        "* Use a fun√ß√£o `torch.triu()` para criar a parte superior da matriz de forma triangular.\n",
        "\n",
        "* Adicione a m√°scara √† matriz de pontua√ß√µes de similaridade (`sims`). O valor `-inf` garantir√° que o softmax transforme a pontua√ß√£o de aten√ß√£o em zero, ignorando os tokens futuros.\n",
        "\n",
        "* Mantenha o restante da sua implementa√ß√£o de self-attention, como a escalabilidade e a multiplica√ß√£o com a matriz de valores, intacto.\n",
        "\n",
        "* Lembre-se que o \"mascaramento\" acontece ao fazer a adi√ß√£o da matriz de m√°scara, conforme a f√≥rmula derivada a seguir:\n",
        "\n",
        "\n",
        "$$\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V$$\n"
      ],
      "metadata": {
        "id": "CHPer5Wzupya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import M\n",
        "# Altere o mesmo c√≥digo que voc√™ fez para a classe SelfAttention para criar a implementa√ß√£o da MaskedAttention.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Uma classe b√°sica de Masked-Attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=2,\n",
        "                 row_dim=0,\n",
        "                 col_dim=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Inicializa os Pesos (W) para queries, keys e values\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "        self.row_dim = row_dim\n",
        "        self.col_dim = col_dim\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, token_encodings):\n",
        "        ## SEU C√ìDIGO AQUI!\n",
        "        ## Crie queries, keys e values.\n",
        "        q = self.W_q(token_encodings)\n",
        "        k = self.W_k(token_encodings)\n",
        "        v = self.W_v(token_encodings)\n",
        "\n",
        "        ## Calcule as pontua√ß√µes de similaridade.\n",
        "        #transposta de k\n",
        "        k_t = k.transpose(0,1)\n",
        "\n",
        "        ## Compute as pontua√ß√µes de similaridade.\n",
        "        sims = torch.matmul(q,k_t)\n",
        "\n",
        "        #Cria m√°scara causal M com 0 na diagonal/abaixo e -inf acima\n",
        "        #Usamos o menor n√∫mero represent√°vel do dtype de sims como -inf aditivo.\n",
        "        neg_inf = torch.finfo(sims.dtype).min\n",
        "        # matriz superior estrita (acima da diagonal) com 1s\n",
        "        upper_tri = torch.triu(torch.ones_like(sims), diagonal=1)\n",
        "        # M: 0 em diag/abaixo, -inf acima\n",
        "        M = upper_tri * neg_inf                 # (T, T)\n",
        "\n",
        "        ## Dimensione as similaridades.\n",
        "        dim_k = k.size(self.col_dim)\n",
        "        masked_scaled_sims = (sims + M) / dim_k ** 0.5\n",
        "\n",
        "        ## Aplique softmax para obter as porcentagens de aten√ß√£o.\n",
        "        perc_atencao = torch.nn.functional.softmax(masked_scaled_sims, dim=self.col_dim)\n",
        "\n",
        "        ## Calcule as pontua√ß√µes finais de aten√ß√£o.\n",
        "        attention_scores = torch.matmul(perc_atencao,v)\n",
        "\n",
        "        return attention_scores"
      ],
      "metadata": {
        "id": "_qip6xU277eA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instru√ß√µes:**\n",
        "\n",
        "Ap√≥s implementar, execute a sua classe MaskedSelfAttention com os mesmos valores de entrada e seed da implementa√ß√£o SelfAttention.\n",
        "\n",
        "A sa√≠da do seu c√≥digo deve corresponder √† sa√≠da esperada abaixo:\n",
        "\n",
        "\n",
        "```\n",
        "tensor([[ 0.6038,  0.7434],\n",
        "        [-0.0062,  0.6072],\n",
        "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "W1XamapX49HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crie uma matriz de codifica√ß√µes de token\n",
        "encodings_matrix2 = torch.tensor([[1.16, 0.23],\n",
        "                                 [0.57, 1.36],\n",
        "                                 [4.41, -2.16]])\n",
        "\n",
        "# Defina a semente para reprodutibilidade\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Crie um objeto de self-attention\n",
        "maskedSelfAttention = MaskedSelfAttention()\n",
        "\n",
        "# Calcule as pontua√ß√µes de aten√ß√£o para as codifica√ß√µes de token\n",
        "maskedSelfAttention(encodings_matrix2)"
      ],
      "metadata": {
        "id": "EzbeQX5O5WSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4cf6434-acf9-4c32-80f4-bdb228739638"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6038,  0.7434],\n",
              "        [-0.0062,  0.6072],\n",
              "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}
